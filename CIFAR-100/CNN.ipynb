{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR-100(CNN)",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkyjSu_V6MEu"
      },
      "source": [
        "# Import requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFRjq3jxScCr",
        "outputId": "92894127-b3c5-42d3-a640-e659fee7031b"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device('cuda')\n",
        "else:\n",
        "    DEVICE = torch.device('cpu')\n",
        "print(f'Device: {DEVICE}')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rx0edS1iylaP",
        "outputId": "0d0ac251-996b-4665-a2ca-38621d9d9562"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Jul 27 21:06:39 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8    10W /  70W |      3MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kO45Icj16BO3"
      },
      "source": [
        "# Download the CIFAR-100 data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMwtNrQySvYW",
        "outputId": "e60ba465-f6d4-4c8a-898a-a53ed6d604a0"
      },
      "source": [
        "train_dataset = datasets.CIFAR100(root='../data/CIFAR100',\n",
        "    train=True,\n",
        "    download=True,\n",
        ")\n",
        "test_dataset = datasets.CIFAR100(root='../data/CIFAR100',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "    ]),\n",
        ")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40_ikEoPGZd_",
        "outputId": "c14a0bbd-96c1-440d-ee0a-768b7740b148"
      },
      "source": [
        "for image, label in train_dataset:\n",
        "    print(image.shape)\n",
        "    print(label)\n",
        "    break\n",
        "\n",
        "for image, label in test_dataset:\n",
        "    print(image.shape)\n",
        "    print(label)\n",
        "    break"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 32, 32])\n",
            "19\n",
            "torch.Size([3, 32, 32])\n",
            "49\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZnchvH8DAUn",
        "outputId": "6def7f41-88bb-4168-ecf3-90895fdafc3c"
      },
      "source": [
        "train_list = []\n",
        "for (image, label) in tqdm(train_dataset):\n",
        "    img0 = image\n",
        "    img1 = transforms.RandomHorizontalFlip(1)(image)\n",
        "    img2 = transforms.RandomRotation(10)(image)\n",
        "    img3 = transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2))(image)\n",
        "    img4 = transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2)(image)\n",
        "    imgs = [img0, img1, img2, img3, img4]\n",
        "    \n",
        "    for img in imgs:\n",
        "        img = transforms.ToTensor()(img).view(-1, 32, 32)\n",
        "        img = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(img)\n",
        "        train_list.append((img, label))\n",
        "\n",
        "print(f'\\n{len(train_list)}')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50000/50000 [01:19<00:00, 631.87it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "250000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiu36033DUIf"
      },
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_list):\n",
        "        self.data = data_list\n",
        "    \n",
        "    def __len__(self):\n",
        "        # size is same as length of list\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # reshape tensor as (3, 32, 32)\n",
        "        image = self.data[idx][0] #.view(3, 32, 32) \n",
        "        label = self.data[idx][1]\n",
        "        return image, label\n",
        "\n",
        "train_dataset = MyDataset(train_list)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3m3hj9nUcio",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41fd0391-ac55-4748-d06b-aa22255619ca"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset=train_dataset, \n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        ")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg37zH3Z6VxK"
      },
      "source": [
        "# Define the Convolution Neural Network (CNN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3jcHCcA_0lb"
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        # (3, 32, 32)\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), padding=(1, 1))\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=(3, 3), padding=(1,1))\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 128, kernel_size=(3, 3), padding=(1,1))\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=(3, 3), padding=(1,1))\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "        self.conv5 = nn.Conv2d(256, 512, kernel_size=(3, 3), padding=(1,1))\n",
        "        self.bn5 = nn.BatchNorm2d(512)\n",
        "        self.conv6 = nn.Conv2d(512, 512, kernel_size=(3,3), padding=(1,1))\n",
        "        self.bn6 = nn.BatchNorm2d(512)\n",
        "        self.conv7 = nn.Conv2d(512, 1024, kernel_size=(3,3), padding=(1,1))\n",
        "        self.bn7 = nn.BatchNorm2d(1024)\n",
        "        self.conv8 = nn.Conv2d(1024, 1024, kernel_size=(3,3), padding=(1,1))\n",
        "        self.bn8 = nn.BatchNorm2d(1024)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.classify = nn.Sequential(\n",
        "            nn.Linear(1024, 100),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input shape: (3, 32, 32)\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool(x) # (128, 16, 16)\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = self.pool(x) # (256, 8, 8)\n",
        "        x = F.relu(self.bn5(self.conv5(x)))\n",
        "        x = self.pool(x) # (512, 4, 4)\n",
        "        x = F.relu(self.bn6(self.conv6(x)))\n",
        "        x = F.relu(self.bn7(self.conv7(x)))\n",
        "        x = self.pool(x) # (1024, 2, 2)\n",
        "        x = F.relu(self.bn8(self.conv8(x)))\n",
        "        x = F.relu(self.bn8(self.conv8(x)))\n",
        "        x = self.pool(x) # (1024, 1, 1)\n",
        "\n",
        "        x = x.view(x.size(0), -1) # 1024\n",
        "        x = self.classify(x)\n",
        "        return x"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Durloa9S6hPO"
      },
      "source": [
        "# Define the train, evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouAyIUJ9Wt5a"
      },
      "source": [
        "def train(model, train_loader, optimizer, log_interval):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    for batch_idx, (image, label) in enumerate(train_loader):\n",
        "        image = image.to(DEVICE)\n",
        "        label = label.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(image)\n",
        "        loss = criterion(output, label)\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (batch_idx + 1) % log_interval == 0:\n",
        "            pct = 100 * batch_idx / len(train_loader) # percent\n",
        "            train_loss /= log_interval\n",
        "            print(f'Train Epoch: {Epoch} [{batch_idx * len(image)}/{len(train_loader.dataset)} ({pct:.0f}%)]\\tAverage Train Loss: {train_loss:.6f}')\n",
        "            train_loss = 0\n",
        "\n",
        "\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for image, label in test_loader:\n",
        "            image = image.to(DEVICE)\n",
        "            label = label.to(DEVICE)\n",
        "            \n",
        "            output = model(image)\n",
        "            test_loss += criterion(output, label).item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(label.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_accuracy = 100 * correct / len(test_loader.dataset)\n",
        "\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrjIlYb16ltY"
      },
      "source": [
        "# set seeds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eG7t0-5vMzyi"
      },
      "source": [
        "def fix_seeds(seed = 42, use_torch=False):\n",
        "    # fix the seed for reproducibility \n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    if use_torch: \n",
        "        torch.manual_seed(seed) \n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1aOKVvtM7SV"
      },
      "source": [
        "# initialize the weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7bduOeURG9-"
      },
      "source": [
        "def init_weights(m):\n",
        "    # initialize the weight, bias\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        torch.nn.init.kaiming_uniform_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            torch.nn.init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.BatchNorm2d):\n",
        "        torch.nn.init.normal_(m.weight.data, mean=1, std=0.02)\n",
        "        torch.nn.init.constant_(m.bias.data, 0)\n",
        "    elif isinstance(m, nn.Linear):\n",
        "        torch.nn.init.kaiming_uniform_(m.weight.data)\n",
        "        torch.nn.init.normal_(m.bias.data)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ_GaCMiM_A0"
      },
      "source": [
        "# Train & Test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zvfG337EbK51",
        "outputId": "0307f12a-b5b3-4e53-bf4e-4ffe62873246"
      },
      "source": [
        "SEED = 42\n",
        "EPOCHS = 100\n",
        "\n",
        "fix_seeds(seed=SEED, use_torch=True)\n",
        "model = ConvNet().to(device=DEVICE)\n",
        "model.apply(init_weights)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "for Epoch in range(1, EPOCHS + 1):\n",
        "    train(model, train_loader, optimizer, log_interval=int(len(train_loader) * 0.2))\n",
        "    test_loss, test_acc = evaluate(model, test_loader)\n",
        "    scheduler.step(test_loss)\n",
        "\n",
        "    print(f'\\nEpoch: {Epoch}')\n",
        "    print(f'Average Test Loss: {test_loss:.4f}')\n",
        "    print(f'Test Accuracy: {test_acc:.2f} %\\n')\n",
        "    #torch.save(model, f'./models/model_{Epoch:02d}.pt')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [49952/250000 (20%)]\tAverage Train Loss: 4.174800\n",
            "Train Epoch: 1 [99936/250000 (40%)]\tAverage Train Loss: 3.076969\n",
            "Train Epoch: 1 [149920/250000 (60%)]\tAverage Train Loss: 2.473030\n",
            "Train Epoch: 1 [199904/250000 (80%)]\tAverage Train Loss: 2.129765\n",
            "Train Epoch: 1 [249888/250000 (100%)]\tAverage Train Loss: 1.833055\n",
            "\n",
            "Epoch: 1\n",
            "Average Test Loss: 0.1112\n",
            "Test Accuracy: 25.03 %\n",
            "\n",
            "Train Epoch: 2 [49952/250000 (20%)]\tAverage Train Loss: 1.561672\n",
            "Train Epoch: 2 [99936/250000 (40%)]\tAverage Train Loss: 1.395788\n",
            "Train Epoch: 2 [149920/250000 (60%)]\tAverage Train Loss: 1.249112\n",
            "Train Epoch: 2 [199904/250000 (80%)]\tAverage Train Loss: 1.118763\n",
            "Train Epoch: 2 [249888/250000 (100%)]\tAverage Train Loss: 0.993196\n",
            "\n",
            "Epoch: 2\n",
            "Average Test Loss: 0.1084\n",
            "Test Accuracy: 33.67 %\n",
            "\n",
            "Train Epoch: 3 [49952/250000 (20%)]\tAverage Train Loss: 0.724231\n",
            "Train Epoch: 3 [99936/250000 (40%)]\tAverage Train Loss: 0.670889\n",
            "Train Epoch: 3 [149920/250000 (60%)]\tAverage Train Loss: 0.601991\n",
            "Train Epoch: 3 [199904/250000 (80%)]\tAverage Train Loss: 0.553835\n",
            "Train Epoch: 3 [249888/250000 (100%)]\tAverage Train Loss: 0.508139\n",
            "\n",
            "Epoch: 3\n",
            "Average Test Loss: 0.0977\n",
            "Test Accuracy: 39.23 %\n",
            "\n",
            "Train Epoch: 4 [49952/250000 (20%)]\tAverage Train Loss: 0.317840\n",
            "Train Epoch: 4 [99936/250000 (40%)]\tAverage Train Loss: 0.324581\n",
            "Train Epoch: 4 [149920/250000 (60%)]\tAverage Train Loss: 0.309272\n",
            "Train Epoch: 4 [199904/250000 (80%)]\tAverage Train Loss: 0.301819\n",
            "Train Epoch: 4 [249888/250000 (100%)]\tAverage Train Loss: 0.279962\n",
            "\n",
            "Epoch: 4\n",
            "Average Test Loss: 0.0858\n",
            "Test Accuracy: 46.85 %\n",
            "\n",
            "Train Epoch: 5 [49952/250000 (20%)]\tAverage Train Loss: 0.182706\n",
            "Train Epoch: 5 [99936/250000 (40%)]\tAverage Train Loss: 0.196679\n",
            "Train Epoch: 5 [149920/250000 (60%)]\tAverage Train Loss: 0.204003\n",
            "Train Epoch: 5 [199904/250000 (80%)]\tAverage Train Loss: 0.194500\n",
            "Train Epoch: 5 [249888/250000 (100%)]\tAverage Train Loss: 0.200984\n",
            "\n",
            "Epoch: 5\n",
            "Average Test Loss: 0.0839\n",
            "Test Accuracy: 46.73 %\n",
            "\n",
            "Train Epoch: 6 [49952/250000 (20%)]\tAverage Train Loss: 0.130186\n",
            "Train Epoch: 6 [99936/250000 (40%)]\tAverage Train Loss: 0.146447\n",
            "Train Epoch: 6 [149920/250000 (60%)]\tAverage Train Loss: 0.152824\n",
            "Train Epoch: 6 [199904/250000 (80%)]\tAverage Train Loss: 0.153841\n",
            "Train Epoch: 6 [249888/250000 (100%)]\tAverage Train Loss: 0.153161\n",
            "\n",
            "Epoch: 6\n",
            "Average Test Loss: 0.0850\n",
            "Test Accuracy: 48.43 %\n",
            "\n",
            "Train Epoch: 7 [49952/250000 (20%)]\tAverage Train Loss: 0.102250\n",
            "Train Epoch: 7 [99936/250000 (40%)]\tAverage Train Loss: 0.120836\n",
            "Train Epoch: 7 [149920/250000 (60%)]\tAverage Train Loss: 0.118997\n",
            "Train Epoch: 7 [199904/250000 (80%)]\tAverage Train Loss: 0.130577\n",
            "Train Epoch: 7 [249888/250000 (100%)]\tAverage Train Loss: 0.118140\n",
            "\n",
            "Epoch: 7\n",
            "Average Test Loss: 0.0800\n",
            "Test Accuracy: 50.52 %\n",
            "\n",
            "Train Epoch: 8 [49952/250000 (20%)]\tAverage Train Loss: 0.088490\n",
            "Train Epoch: 8 [99936/250000 (40%)]\tAverage Train Loss: 0.100199\n",
            "Train Epoch: 8 [149920/250000 (60%)]\tAverage Train Loss: 0.105951\n",
            "Train Epoch: 8 [199904/250000 (80%)]\tAverage Train Loss: 0.107200\n",
            "Train Epoch: 8 [249888/250000 (100%)]\tAverage Train Loss: 0.105383\n",
            "\n",
            "Epoch: 8\n",
            "Average Test Loss: 0.0874\n",
            "Test Accuracy: 49.68 %\n",
            "\n",
            "Train Epoch: 9 [49952/250000 (20%)]\tAverage Train Loss: 0.079342\n",
            "Train Epoch: 9 [99936/250000 (40%)]\tAverage Train Loss: 0.091074\n",
            "Train Epoch: 9 [149920/250000 (60%)]\tAverage Train Loss: 0.088279\n",
            "Train Epoch: 9 [199904/250000 (80%)]\tAverage Train Loss: 0.095588\n",
            "Train Epoch: 9 [249888/250000 (100%)]\tAverage Train Loss: 0.092785\n",
            "\n",
            "Epoch: 9\n",
            "Average Test Loss: 0.0885\n",
            "Test Accuracy: 48.53 %\n",
            "\n",
            "Train Epoch: 10 [49952/250000 (20%)]\tAverage Train Loss: 0.072855\n",
            "Train Epoch: 10 [99936/250000 (40%)]\tAverage Train Loss: 0.078504\n",
            "Train Epoch: 10 [149920/250000 (60%)]\tAverage Train Loss: 0.082184\n",
            "Train Epoch: 10 [199904/250000 (80%)]\tAverage Train Loss: 0.083504\n",
            "Train Epoch: 10 [249888/250000 (100%)]\tAverage Train Loss: 0.082081\n",
            "\n",
            "Epoch: 10\n",
            "Average Test Loss: 0.0858\n",
            "Test Accuracy: 50.71 %\n",
            "\n",
            "Train Epoch: 11 [49952/250000 (20%)]\tAverage Train Loss: 0.065202\n",
            "Train Epoch: 11 [99936/250000 (40%)]\tAverage Train Loss: 0.075209\n",
            "Train Epoch: 11 [149920/250000 (60%)]\tAverage Train Loss: 0.076009\n",
            "Train Epoch: 11 [199904/250000 (80%)]\tAverage Train Loss: 0.070753\n",
            "Train Epoch: 11 [249888/250000 (100%)]\tAverage Train Loss: 0.080478\n",
            "\n",
            "Epoch: 11\n",
            "Average Test Loss: 0.0975\n",
            "Test Accuracy: 48.00 %\n",
            "\n",
            "Train Epoch: 12 [49952/250000 (20%)]\tAverage Train Loss: 0.057673\n",
            "Train Epoch: 12 [99936/250000 (40%)]\tAverage Train Loss: 0.070909\n",
            "Train Epoch: 12 [149920/250000 (60%)]\tAverage Train Loss: 0.067812\n",
            "Train Epoch: 12 [199904/250000 (80%)]\tAverage Train Loss: 0.068244\n",
            "Train Epoch: 12 [249888/250000 (100%)]\tAverage Train Loss: 0.068966\n",
            "\n",
            "Epoch: 12\n",
            "Average Test Loss: 0.0908\n",
            "Test Accuracy: 49.35 %\n",
            "\n",
            "Train Epoch: 13 [49952/250000 (20%)]\tAverage Train Loss: 0.059874\n",
            "Train Epoch: 13 [99936/250000 (40%)]\tAverage Train Loss: 0.061645\n",
            "Train Epoch: 13 [149920/250000 (60%)]\tAverage Train Loss: 0.066234\n",
            "Train Epoch: 13 [199904/250000 (80%)]\tAverage Train Loss: 0.065358\n",
            "Train Epoch: 13 [249888/250000 (100%)]\tAverage Train Loss: 0.062206\n",
            "\n",
            "Epoch: 13\n",
            "Average Test Loss: 0.0905\n",
            "Test Accuracy: 50.59 %\n",
            "\n",
            "Train Epoch: 14 [49952/250000 (20%)]\tAverage Train Loss: 0.023969\n",
            "Train Epoch: 14 [99936/250000 (40%)]\tAverage Train Loss: 0.016988\n",
            "Train Epoch: 14 [149920/250000 (60%)]\tAverage Train Loss: 0.011627\n",
            "Train Epoch: 14 [199904/250000 (80%)]\tAverage Train Loss: 0.009248\n",
            "Train Epoch: 14 [249888/250000 (100%)]\tAverage Train Loss: 0.010096\n",
            "\n",
            "Epoch: 14\n",
            "Average Test Loss: 0.0863\n",
            "Test Accuracy: 53.04 %\n",
            "\n",
            "Train Epoch: 15 [49952/250000 (20%)]\tAverage Train Loss: 0.004556\n",
            "Train Epoch: 15 [99936/250000 (40%)]\tAverage Train Loss: 0.004467\n",
            "Train Epoch: 15 [149920/250000 (60%)]\tAverage Train Loss: 0.004030\n",
            "Train Epoch: 15 [199904/250000 (80%)]\tAverage Train Loss: 0.004403\n",
            "Train Epoch: 15 [249888/250000 (100%)]\tAverage Train Loss: 0.004151\n",
            "\n",
            "Epoch: 15\n",
            "Average Test Loss: 0.0896\n",
            "Test Accuracy: 54.25 %\n",
            "\n",
            "Train Epoch: 16 [49952/250000 (20%)]\tAverage Train Loss: 0.002580\n",
            "Train Epoch: 16 [99936/250000 (40%)]\tAverage Train Loss: 0.002641\n",
            "Train Epoch: 16 [149920/250000 (60%)]\tAverage Train Loss: 0.002572\n",
            "Train Epoch: 16 [199904/250000 (80%)]\tAverage Train Loss: 0.003185\n",
            "Train Epoch: 16 [249888/250000 (100%)]\tAverage Train Loss: 0.002908\n",
            "\n",
            "Epoch: 16\n",
            "Average Test Loss: 0.0934\n",
            "Test Accuracy: 53.68 %\n",
            "\n",
            "Train Epoch: 17 [49952/250000 (20%)]\tAverage Train Loss: 0.002320\n",
            "Train Epoch: 17 [99936/250000 (40%)]\tAverage Train Loss: 0.001810\n",
            "Train Epoch: 17 [149920/250000 (60%)]\tAverage Train Loss: 0.001993\n",
            "Train Epoch: 17 [199904/250000 (80%)]\tAverage Train Loss: 0.002406\n",
            "Train Epoch: 17 [249888/250000 (100%)]\tAverage Train Loss: 0.002531\n",
            "\n",
            "Epoch: 17\n",
            "Average Test Loss: 0.0922\n",
            "Test Accuracy: 54.03 %\n",
            "\n",
            "Train Epoch: 18 [49952/250000 (20%)]\tAverage Train Loss: 0.001392\n",
            "Train Epoch: 18 [99936/250000 (40%)]\tAverage Train Loss: 0.001628\n",
            "Train Epoch: 18 [149920/250000 (60%)]\tAverage Train Loss: 0.001730\n",
            "Train Epoch: 18 [199904/250000 (80%)]\tAverage Train Loss: 0.002476\n",
            "Train Epoch: 18 [249888/250000 (100%)]\tAverage Train Loss: 0.002439\n",
            "\n",
            "Epoch: 18\n",
            "Average Test Loss: 0.0935\n",
            "Test Accuracy: 54.24 %\n",
            "\n",
            "Train Epoch: 19 [49952/250000 (20%)]\tAverage Train Loss: 0.001446\n",
            "Train Epoch: 19 [99936/250000 (40%)]\tAverage Train Loss: 0.001668\n",
            "Train Epoch: 19 [149920/250000 (60%)]\tAverage Train Loss: 0.001328\n",
            "Train Epoch: 19 [199904/250000 (80%)]\tAverage Train Loss: 0.001680\n",
            "Train Epoch: 19 [249888/250000 (100%)]\tAverage Train Loss: 0.001470\n",
            "\n",
            "Epoch: 19\n",
            "Average Test Loss: 0.0928\n",
            "Test Accuracy: 54.75 %\n",
            "\n",
            "Train Epoch: 20 [49952/250000 (20%)]\tAverage Train Loss: 0.001184\n",
            "Train Epoch: 20 [99936/250000 (40%)]\tAverage Train Loss: 0.000985\n",
            "Train Epoch: 20 [149920/250000 (60%)]\tAverage Train Loss: 0.001167\n",
            "Train Epoch: 20 [199904/250000 (80%)]\tAverage Train Loss: 0.001145\n",
            "Train Epoch: 20 [249888/250000 (100%)]\tAverage Train Loss: 0.001119\n",
            "\n",
            "Epoch: 20\n",
            "Average Test Loss: 0.0952\n",
            "Test Accuracy: 55.35 %\n",
            "\n",
            "Train Epoch: 21 [49952/250000 (20%)]\tAverage Train Loss: 0.000785\n",
            "Train Epoch: 21 [99936/250000 (40%)]\tAverage Train Loss: 0.001008\n",
            "Train Epoch: 21 [149920/250000 (60%)]\tAverage Train Loss: 0.000933\n",
            "Train Epoch: 21 [199904/250000 (80%)]\tAverage Train Loss: 0.000924\n",
            "Train Epoch: 21 [249888/250000 (100%)]\tAverage Train Loss: 0.000744\n",
            "\n",
            "Epoch: 21\n",
            "Average Test Loss: 0.0919\n",
            "Test Accuracy: 55.49 %\n",
            "\n",
            "Train Epoch: 22 [49952/250000 (20%)]\tAverage Train Loss: 0.000614\n",
            "Train Epoch: 22 [99936/250000 (40%)]\tAverage Train Loss: 0.000937\n",
            "Train Epoch: 22 [149920/250000 (60%)]\tAverage Train Loss: 0.000784\n",
            "Train Epoch: 22 [199904/250000 (80%)]\tAverage Train Loss: 0.000826\n",
            "Train Epoch: 22 [249888/250000 (100%)]\tAverage Train Loss: 0.000856\n",
            "\n",
            "Epoch: 22\n",
            "Average Test Loss: 0.0956\n",
            "Test Accuracy: 55.27 %\n",
            "\n",
            "Train Epoch: 23 [49952/250000 (20%)]\tAverage Train Loss: 0.000691\n",
            "Train Epoch: 23 [99936/250000 (40%)]\tAverage Train Loss: 0.000649\n",
            "Train Epoch: 23 [149920/250000 (60%)]\tAverage Train Loss: 0.000693\n",
            "Train Epoch: 23 [199904/250000 (80%)]\tAverage Train Loss: 0.000729\n",
            "Train Epoch: 23 [249888/250000 (100%)]\tAverage Train Loss: 0.000548\n",
            "\n",
            "Epoch: 23\n",
            "Average Test Loss: 0.0943\n",
            "Test Accuracy: 55.58 %\n",
            "\n",
            "Train Epoch: 24 [49952/250000 (20%)]\tAverage Train Loss: 0.000510\n",
            "Train Epoch: 24 [99936/250000 (40%)]\tAverage Train Loss: 0.000850\n",
            "Train Epoch: 24 [149920/250000 (60%)]\tAverage Train Loss: 0.000490\n",
            "Train Epoch: 24 [199904/250000 (80%)]\tAverage Train Loss: 0.000666\n",
            "Train Epoch: 24 [249888/250000 (100%)]\tAverage Train Loss: 0.000657\n",
            "\n",
            "Epoch: 24\n",
            "Average Test Loss: 0.0960\n",
            "Test Accuracy: 54.90 %\n",
            "\n",
            "Train Epoch: 25 [49952/250000 (20%)]\tAverage Train Loss: 0.000548\n",
            "Train Epoch: 25 [99936/250000 (40%)]\tAverage Train Loss: 0.000499\n",
            "Train Epoch: 25 [149920/250000 (60%)]\tAverage Train Loss: 0.000619\n",
            "Train Epoch: 25 [199904/250000 (80%)]\tAverage Train Loss: 0.000703\n",
            "Train Epoch: 25 [249888/250000 (100%)]\tAverage Train Loss: 0.000645\n",
            "\n",
            "Epoch: 25\n",
            "Average Test Loss: 0.0948\n",
            "Test Accuracy: 55.85 %\n",
            "\n",
            "Train Epoch: 26 [49952/250000 (20%)]\tAverage Train Loss: 0.000558\n",
            "Train Epoch: 26 [99936/250000 (40%)]\tAverage Train Loss: 0.000561\n",
            "Train Epoch: 26 [149920/250000 (60%)]\tAverage Train Loss: 0.000567\n",
            "Train Epoch: 26 [199904/250000 (80%)]\tAverage Train Loss: 0.000447\n",
            "Train Epoch: 26 [249888/250000 (100%)]\tAverage Train Loss: 0.000672\n",
            "\n",
            "Epoch: 26\n",
            "Average Test Loss: 0.0940\n",
            "Test Accuracy: 55.89 %\n",
            "\n",
            "Train Epoch: 27 [49952/250000 (20%)]\tAverage Train Loss: 0.000352\n",
            "Train Epoch: 27 [99936/250000 (40%)]\tAverage Train Loss: 0.000554\n",
            "Train Epoch: 27 [149920/250000 (60%)]\tAverage Train Loss: 0.000667\n",
            "Train Epoch: 27 [199904/250000 (80%)]\tAverage Train Loss: 0.000426\n",
            "Train Epoch: 27 [249888/250000 (100%)]\tAverage Train Loss: 0.000509\n",
            "\n",
            "Epoch: 27\n",
            "Average Test Loss: 0.0958\n",
            "Test Accuracy: 55.19 %\n",
            "\n",
            "Train Epoch: 28 [49952/250000 (20%)]\tAverage Train Loss: 0.000438\n",
            "Train Epoch: 28 [99936/250000 (40%)]\tAverage Train Loss: 0.000620\n",
            "Train Epoch: 28 [149920/250000 (60%)]\tAverage Train Loss: 0.000483\n",
            "Train Epoch: 28 [199904/250000 (80%)]\tAverage Train Loss: 0.000402\n",
            "Train Epoch: 28 [249888/250000 (100%)]\tAverage Train Loss: 0.000475\n",
            "\n",
            "Epoch: 28\n",
            "Average Test Loss: 0.0953\n",
            "Test Accuracy: 55.17 %\n",
            "\n",
            "Train Epoch: 29 [49952/250000 (20%)]\tAverage Train Loss: 0.000525\n",
            "Train Epoch: 29 [99936/250000 (40%)]\tAverage Train Loss: 0.000406\n",
            "Train Epoch: 29 [149920/250000 (60%)]\tAverage Train Loss: 0.000640\n",
            "Train Epoch: 29 [199904/250000 (80%)]\tAverage Train Loss: 0.000554\n",
            "Train Epoch: 29 [249888/250000 (100%)]\tAverage Train Loss: 0.000421\n",
            "\n",
            "Epoch: 29\n",
            "Average Test Loss: 0.0932\n",
            "Test Accuracy: 54.67 %\n",
            "\n",
            "Train Epoch: 30 [49952/250000 (20%)]\tAverage Train Loss: 0.000622\n",
            "Train Epoch: 30 [99936/250000 (40%)]\tAverage Train Loss: 0.000412\n",
            "Train Epoch: 30 [149920/250000 (60%)]\tAverage Train Loss: 0.000538\n",
            "Train Epoch: 30 [199904/250000 (80%)]\tAverage Train Loss: 0.000625\n",
            "Train Epoch: 30 [249888/250000 (100%)]\tAverage Train Loss: 0.000431\n",
            "\n",
            "Epoch: 30\n",
            "Average Test Loss: 0.0932\n",
            "Test Accuracy: 55.03 %\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-8378907481c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mEpoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-3c2bce8d2bc4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, log_interval)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeNlX8dbt5tK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
